# -*- coding: utf-8 -*-
"""ruroberta_markers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ib7ctJquAxTnzeQt7424CwadayAtgAmQ

#обучение
"""

# !pip install datasets tokenizers transformers evaluate

from datasets import Dataset, Features, Value, ClassLabel, Sequence

true_labels = ["O", "B-MRK", "I-MRK"]

# Функция для чтения данных из файла
def read_ner_file(file_path, labels):
    tokens, tags = [], []
    current_tokens, current_tags = [], []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                if current_tokens:
                    tokens.append(current_tokens)
                    tags.append(current_tags)
                    current_tokens, current_tags = [], []
            else:
                parts = line.split()
                if len(parts) == 2:
                    current_tokens.append(parts[0])
                    current_tags.append(parts[1])

                    if parts[1] not in labels:
                      labels.append(parts[1])
                      labels.append(f'I-{parts[1][2:]}')

    return {"tokens": tokens, "ner_tags": tags}

# Создаем Dataset
data = read_ner_file("ner_tags_train.txt", true_labels)
dataset = Dataset.from_dict({
    "tokens": data["tokens"],
    "ner_tags": data["ner_tags"]
})

data1 = read_ner_file("ner_tags_test.txt", true_labels)
dataset1 = Dataset.from_dict({
    "tokens": data1["tokens"],
    "ner_tags": data1["ner_tags"]
})

print(true_labels)

from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification

from transformers import AutoTokenizer

model_name = "ai-forever/ruRoberta-large"
tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True, num_labels=len(true_labels))

# Создаем соответствие меток и их ID
label2id = {label: i for i, label in enumerate(true_labels)}
id2label = {i: label for i, label in enumerate(true_labels)}

# Обновляем датасет с ID меток
def align_labels(examples, label_all_tokens=True):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True, max_length=512, padding="max_length")
    # tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)
    labels = []
    for i, tags in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Игнорируем специальные токены
            elif word_idx != previous_word_idx:
                label_ids.append(label2id[tags[word_idx]])
            else:
                label_ids.append(label2id[tags[word_idx]] if label_all_tokens else -100)
            previous_word_idx = word_idx
        labels.append(label_ids)
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

tokenized_dataset = dataset.map(align_labels, batched=True)
tokenized_dataset1 = dataset1.map(align_labels, batched=True)

for sample in tokenized_dataset1:
  if len(sample["input_ids"]) != len(sample["labels"]):
    print(len(sample["input_ids"]), len(sample["labels"]))

from transformers import TrainingArguments, Trainer

model = AutoModelForTokenClassification.from_pretrained(
    model_name,
    num_labels=len(true_labels),
    id2label=id2label,
    label2id=label2id
)

args = TrainingArguments(
    "argmark-ner",
    learning_rate = 2e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=2,
    weight_decay=0.01,
    eval_strategy = "epoch",
    fp16=False
    # save_strategy="epoch",
    # load_best_model_at_end=True
)

data_collator = DataCollatorForTokenClassification(tokenizer)

# !pip install seqeval

import evaluate
import numpy as np
metric = evaluate.load("seqeval")

def compute_metrics(eval_preds):
  pred_logits, labels = eval_preds
  pred_logits = np.argmax(pred_logits, axis = 2)
  predictions = [
      [true_labels[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100] for prediction, label in zip(pred_logits, labels)
  ]
  labels1 = [
      [true_labels[l] for (eval_preds, l) in zip(prediction, label) if l != -100] for prediction, label in zip(pred_logits, labels)
  ]
  results = metric.compute(predictions=predictions, references=labels1)
  return {
      "precision": results['overall_precision'],
      "recall": results['overall_recall'],
      "f1":results['overall_f1'],
      "accuracy": results['overall_accuracy']

  }

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset1,
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

model.config.label2id = label2id
model.config.id2label = id2label


model.save_pretrained("argmrkner_model")
tokenizer.save_pretrained("argmrkner_tknzr")

from google.colab import drive
drive.mount('/content/drive')